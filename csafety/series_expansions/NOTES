13:22 < nsz> simple implementation of math functions is possible, but libm has to satisfy a lot of non-trivial requirements (fenv, low 
             ulp error in the entire range, error handling,..)

13:25 < nsz> usually it's special case handling, arg reduction, polynomial approximation, recover the result

13:27 < nsz> the special case handling can be non-obvious for a libm, if you need low ulp errors then the rest may need some tricks 
             (for 1ulp errors often extra precision is needed so some multi prec arithmetics is used somewhere)


13:37 < nsz> there are many ways, normal scalar functions can use branches and table lookups, in gpu or simd code you dont want those
13:38 < nsz> extra precision is e.g. needed for trig arg reduction (to do mod pi/2) and in log to do avoid cancellation in the final 
             add (x=a*b -> log(x) = log(a)+log(b) is used)
13:40 < nsz> exp is simple if 1-2 ulp error is acceptable (e.g. e^x = 2^k*2^r for some k and r, use large enough poly for 2^r and 
             bithack for 2^k)

13:48 < nsz> graphitemaster: using minimax polyinomial (with remez algorithm) instead of taylor polynomial for approximation is the 
             most basic aspect of math function implementation

13:50 < nsz> actually remez is not the most optimal way
13:50 < graff> ah i see what nsz said yeah. right they can't really be used. there are some convergence issues among other problems
13:50 < nsz> because of floating point rounding errors
13:50 < nsz> remez gives you optimal result on an interval assuming infinite precise coefficients and infinite precise arithmetics
13:51 < graff> i remember dalias said something about them requiring exploding numbers of terms. i have unearthed an aproximation that 
               blows these up using ratios. perhaps related tothe polynomial stuff, but so far it just looks like optimized taylor
13:51 < nsz> usually ppl use remez then trucate the coeffs which gives worse result than carefully selecting exactly representible 
             coefficients
13:52 < nsz> but the difference is something like 1.4 worst-case ulp error instead of 1.6 ulp (but that is 1 ulp vs 2 ulp after 
             rounding)
13:53 < nsz> so it's not huge but may matter (also you can optimize for the number of misrounded cases, but only using heuristics, 
             there is no algorithm that efficiently computes the best coeffs)

13:56 < nsz> ppl often use the remez coeffs directly, those are independent of floating-point representation, only depend on the 
             interval and polynomial degree
13:57 < nsz> but you can do better, especially for single prec, where it's easy to try all inputs in the approximated range
13:57 < graphitemaster> I bet there is hundreds of things we simply don't know we can do because the mathematics / physics / science 
                        behind it (which already exists) is unreachable because no one has made the connections yet
13:57 < graff> graphitemaster:  think that taylor series is good for teaching programmers how to read summation notation if they don't 
               already know
13:57 < graphitemaster> probably hundreds of thousands of things even
13:57 < graff> but indeed i am missing some of the steps
13:58 < graff> i am hoping this paper by Spigot will help
13:58 < nsz> (and if you optimize the coeffs with bruteforce, then they will depend on the precision as well as the exact arithmetics 
             used such as fma vs mul+add)
13:58 < graphitemaster> dude, they teach summation notation in high school.
13:58 < graff> he uses a lot of very to easy understand terminology and examples
13:59 < graff> graphitemaster: yeah but relating that to the structure of a foor loop idiom is not always obvious to everyone
13:59 < graphitemaster> I remember the first time in highschool I actually learned summation notation, we had some weird
13:59 < graff> not to mention, a lot of people need to relearn it. regardless though i get your point
14:00 < graphitemaster> 1/(1*2) + 1/(2*3) + 1/(3*4) ... + 1/(100*101)
14:00 < nsz> for double prec i'd probably use either remez coeffs directly or the coeffs computed by the sollya tool (it can use 
             additional heuristics to tune for representation)
14:01 < nsz> most single prec functions are faster to implement using double prec arithmetics (but again it's only useful for non-simd 
             implementations)
14:02 < graphitemaster> and the teacher is like, yeah that's just \sum_{k=3}^{10}2k+1
14:02 < graff> :p
14:03 < nsz> but if you use single prec arithmetics then it's possible to bruteforce optimize the coeffs for particular property (to 
             get slightly better results from the same algorithm by only changing the coeffs a bit)
13:58 < nsz> (and if you optimize the coeffs with bruteforce, then they will depend on the precision as well as the exact arithmetics 
             used such as fma vs mul+add)

